import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from pathlib import Path
import os
import requests

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CHUNKS_PATH = "dataset/all_chunks.json"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
QA_MODEL_NAME = "AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru"
INDEX_PATH = "dataset/faiss_index.bin"
MAX_CONTEXT_LENGTH = 3000
TOP_K_CHUNKS = 5

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
EMBEDDING_DIMENSION = 768  # –î–ª—è all-mpnet-base-v2 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 768

def download_model_with_fallback(model_name, local_dir):
    """–ü—ã—Ç–∞–µ—Ç—Å—è —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
    from huggingface_hub import snapshot_download, HfApi
    
    try:
        snapshot_download(
            repo_id=model_name,
            local_dir=local_dir,
            local_dir_use_symlinks=False
        )
        print(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞")
        return True
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {model_name}: {e}")
        print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è...")
        try:
            api = HfApi()
            files = api.list_files_info(model_name)
            
            os.makedirs(local_dir, exist_ok=True)
            
            for file in files:
                print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file.rfilename}...")
                url = f"https://huggingface.co/{model_name}/resolve/main/{file.rfilename}"
                response = requests.get(url, stream=True)
                
                file_path = os.path.join(local_dir, file.rfilename)
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                
                with open(file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
            
            return True
        except Exception as e2:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e2}")
            return False

def load_chunks(chunks_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
    if not os.path.exists(chunks_path):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω {chunks_path}")
        return []
    
    with open(chunks_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    return chunks

def create_faiss_index(chunks, model, index_path):
    """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é"""
    texts = [chunk["text"] for chunk in chunks]
    print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    embeddings = model.encode(texts, show_progress_bar=True)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞—Ä–∞–Ω–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    dimension = EMBEDDING_DIMENSION
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings).astype('float32'))
    faiss.write_index(index, index_path)
    print(f"FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {index_path} (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension})")
    return index

def load_or_create_index(chunks, model, index_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
    if Path(index_path).exists():
        try:
            index = faiss.read_index(index_path)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞
            if index.d == EMBEDDING_DIMENSION:
                print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {index.d})")
                return index
            else:
                print(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏! –ò–Ω–¥–µ–∫—Å: {index.d}, –û–∂–∏–¥–∞–µ—Ç—Å—è: {EMBEDDING_DIMENSION}")
                print("–ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å...")
                os.remove(index_path)  # –£–¥–∞–ª—è–µ–º –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω–¥–µ–∫—Å
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            print("–ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å...")
            os.remove(index_path)
    
    return create_faiss_index(chunks, model, index_path)

def find_relevant_chunks(question, embedding_model, index, chunks, top_k=5):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
    question_embedding = embedding_model.encode([question])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞
    if question_embedding.shape[1] != EMBEDDING_DIMENSION:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞ ({question_embedding.shape[1]}) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π ({EMBEDDING_DIMENSION})")
        # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å - –æ–±—Ä–µ–∑–∞–µ–º/–¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if question_embedding.shape[1] > EMBEDDING_DIMENSION:
            question_embedding = question_embedding[:, :EMBEDDING_DIMENSION]
        else:
            padding = np.zeros((1, EMBEDDING_DIMENSION - question_embedding.shape[1]))
            question_embedding = np.hstack([question_embedding, padding])
    
    distances, indices = index.search(question_embedding, top_k)
    return [chunks[i] for i in indices[0]]

def main():
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(CHUNKS_PATH):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {CHUNKS_PATH}")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
    chunks = load_chunks(CHUNKS_PATH)
    if not chunks:
        print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL_NAME}")
    
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é
        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ Hugging Face Hub")
    except Exception as e:
        print(f"–ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        print("–ü—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ...")
        local_path = f"models/{EMBEDDING_MODEL_NAME.split('/')[-1]}"
        
        if download_model_with_fallback(EMBEDDING_MODEL_NAME, local_path):
            embedding_model = SentenceTransformer(local_path)
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    model_dim = embedding_model.get_sentence_embedding_dimension()
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_dim}")
    
    # –°–æ–∑–¥–∞–µ–º/–∑–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
    index = load_or_create_index(chunks, embedding_model, INDEX_PATH)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º QA-–º–æ–¥–µ–ª—å
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ QA-–º–æ–¥–µ–ª–∏: {QA_MODEL_NAME}")
    try:
        qa_pipeline = pipeline(
            "question-answering",
            model=QA_MODEL_NAME,
            tokenizer=QA_MODEL_NAME,
            device=-1
        )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        print("–ü—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ...")
        local_qa_path = f"models/{QA_MODEL_NAME.split('/')[-1]}"
        
        if download_model_with_fallback(QA_MODEL_NAME, local_qa_path):
            qa_pipeline = pipeline(
                "question-answering",
                model=local_qa_path,
                tokenizer=local_qa_path,
                device=-1
            )
        else:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç")
            qa_pipeline = pipeline(
                "question-answering",
                model="distilbert-base-cased-distilled-squad",
                tokenizer="distilbert-base-cased-distilled-squad",
                device=-1
            )
    
    print("\n–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ! –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å...")
    
    while True:
        question = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        if question.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
            break
            
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        relevant_chunks = find_relevant_chunks(
            question, embedding_model, index, chunks, TOP_K_CHUNKS
        )
        if not relevant_chunks:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            continue
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = "\n\n".join([chunk["text"] for chunk in relevant_chunks])
        if len(context) > MAX_CONTEXT_LENGTH:
            print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â–µ–Ω —Å {len(context)} –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤")
            context = context[:MAX_CONTEXT_LENGTH]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        try:
            results = qa_pipeline(
                question=question,
                context=context,
                top_k=3
            )
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç
            best_result = max(results, key=lambda x: x['score'])
            answer = best_result['answer']
            confidence = best_result['score']
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if confidence < 0.1:
                answer = "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö."
            elif len(answer) < 20:  # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –±–æ–ª–µ–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç
                for result in results:
                    if len(result['answer']) > len(answer):
                        answer = result['answer']
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            answer = "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –≤–æ–ø—Ä–æ—Å."
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nü§ñ –û—Ç–≤–µ—Ç: {answer}")
        # if confidence >= 0.1:
            # print(f"üîí –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
        # print("\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
        # for i, chunk in enumerate(relevant_chunks):
            # print(f"{i+1}. {chunk['source']}")

if __name__ == "__main__":
    main()